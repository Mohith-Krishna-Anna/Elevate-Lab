**Real-Time Sign Language Recognition**

This project uses deep learning and computer vision to recognize American Sign Language (ASL) alphabet gestures in real time using a webcam. It helps facilitate communication for the hearing and speech impaired by converting hand signs into predicted letters.


**Features**

- Trained CNN model using Sign Language MNIST dataset
- Real-time prediction of 24 ASL letters (Aâ€“Z, excluding J and Z)
- Live webcam feed with ROI box for gesture input
- Prediction overlaid on video feed
- Model accuracy over 92%

**Requirements**

- Python
- TensorFlow / Keras
- OpenCV
- NumPy / Pandas
- scikit-learn
- Sign Language MNIST Dataset
